# SoundFlow Pipeline - Terraform Variables
# Copy this file to terraform.tfvars and fill in your values

# Required: Your GCP Project ID
project_id = "your-gcp-project-id"

# GCS bucket name (must be globally unique)
gcs_bucket_name = "soundflow-data-lake-your-unique-suffix"

# Region for resources
region = "asia-southeast1"

# BigQuery location
bq_location = "asia-southeast1"

# Environment (dev, staging, prod)
environment = "dev"

# Optional: Enable Cloud Run services
enable_cloud_run = false

# Optional: Docker images for Cloud Run (required if enable_cloud_run = true)
# dagster_image = "gcr.io/your-project/soundflow-dagster:latest"
# spark_image = "gcr.io/your-project/soundflow-spark:latest"

# ==============================================================================
# DATAPROC CONFIGURATION (Recommended for Spark Streaming)
# ==============================================================================

# Enable Dataproc cluster for Spark Streaming
enable_dataproc = true

# Master node configuration
dataproc_master_machine_type = "n1-standard-4"  # 4 vCPU, 15GB RAM
dataproc_master_disk_size    = 100              # GB

# Worker nodes configuration
dataproc_worker_machine_type = "n1-standard-4"  # 4 vCPU, 15GB RAM
dataproc_worker_disk_size    = 100              # GB
dataproc_num_workers         = 2

# Cost optimization: Use preemptible workers (optional)
dataproc_num_preemptible_workers = 0

# Security: Use internal IP only (recommended for production)
dataproc_internal_ip_only = false

# Autoscaling (optional)
enable_dataproc_autoscaling = false

# Workflow template for batch processing (optional)
enable_workflow_template = false
