# SoundFlow - Local PostgreSQL Setup

HÆ°á»›ng dáº«n cháº¡y pipeline hoÃ n chá»‰nh trÃªn local vá»›i PostgreSQL.

## ğŸ“‹ Prerequisites

- Docker & Docker Compose
- 8GB RAM minimum
- Ports available: 5432, 3000, 8080, 8081, 9092

## ğŸš€ Quick Start

### 1. Start All Services

```bash
docker-compose -f docker-compose.local.yml up -d
```

Services khá»Ÿi Ä‘á»™ng:
- âœ… PostgreSQL (port 5432)
- âœ… Adminer UI (port 8081)
- âœ… Redpanda (port 9092)
- âœ… Redpanda Console (port 8080)
- âœ… EventSim (event generator)
- âœ… Spark Streaming (writes to PostgreSQL every 30s)
- âœ… Dagster (port 3000)

### 2. Monitor Services

**Xem logs:**
```bash
# Táº¥t cáº£ services
docker-compose -f docker-compose.local.yml logs -f

# Chá»‰ Spark
docker-compose -f docker-compose.local.yml logs -f spark-streaming

# Chá»‰ EventSim
docker-compose -f docker-compose.local.yml logs -f eventsim
```

**Check Redpanda topics:**
```bash
docker exec -it redpanda rpk topic list
docker exec -it redpanda rpk topic consume listen_events --num 10
```

### 3. Access Web UIs

| Service | URL | Description |
|---------|-----|-------------|
| **Adminer** | http://localhost:8081 | PostgreSQL Admin UI |
| **Redpanda Console** | http://localhost:8080 | Kafka/Topics Monitor |
| **Dagster** | http://localhost:3000 | Workflow Orchestration |

**Login Adminer:**
- System: `PostgreSQL`
- Server: `postgres`
- Username: `soundflow`
- Password: `soundflow123`
- Database: `soundflow`

### 4. Check PostgreSQL Data

```bash
# Connect to PostgreSQL
docker exec -it postgres psql -U soundflow -d soundflow

# Check tables
\dt raw.*

# Count records
SELECT COUNT(*) FROM raw.listen_events;
SELECT COUNT(*) FROM raw.page_view_events;
SELECT COUNT(*) FROM raw.auth_events;

# View latest listen events
SELECT event_timestamp, first_name, last_name, song, artist, city, state 
FROM raw.listen_events 
ORDER BY event_timestamp DESC 
LIMIT 10;
```

### 5. Run dbt Transformations

```bash
# Enter dbt container (if running)
docker exec -it dagster bash

# Or run dbt locally
cd dbt
dbt debug --profiles-dir .
dbt run --profiles-dir .
dbt test --profiles-dir .
```

Check transformed data:
```sql
-- Staging tables
SELECT * FROM staging.stg_listen_events LIMIT 10;

-- Marts
SELECT * FROM marts.mart_top_songs LIMIT 20;
SELECT * FROM marts.mart_active_users LIMIT 20;
SELECT * FROM marts.mart_location_analytics;
```

## ğŸ” Troubleshooting

**Spark khÃ´ng ghi dá»¯ liá»‡u:**
```bash
# Check Spark logs
docker logs spark-streaming

# Restart Spark
docker-compose -f docker-compose.local.yml restart spark-streaming
```

**PostgreSQL connection refused:**
```bash
# Check PostgreSQL is running
docker exec -it postgres pg_isready -U soundflow

# Check network
docker network inspect data_streaming_pipeline_streaming-network
```

**EventSim stopped:**
```bash
# EventSim generates 7 days of historical data then stops in continuous mode
# Check logs to see if it's still generating
docker logs eventsim

# Restart to generate more data
docker-compose -f docker-compose.local.yml restart eventsim
```

## ğŸ›‘ Stop Services

```bash
# Stop all
docker-compose -f docker-compose.local.yml down

# Stop and remove volumes (âš ï¸ deletes all data)
docker-compose -f docker-compose.local.yml down -v
```

## ğŸ“Š Data Flow

```
EventSim 
  â†“ (JSON events)
Redpanda (4 topics: listen_events, page_view_events, auth_events, status_change_events)
  â†“ (Kafka consumer)
Spark Streaming (micro-batch every 30s)
  â†“ (JDBC write)
PostgreSQL raw schema
  â†“ (dbt transformations)
PostgreSQL staging schema
  â†“ (dbt aggregations)
PostgreSQL marts schema
  â†“
Dashboard / Analytics
```

## ğŸ¯ Next Steps

1. âœ… Verify data in PostgreSQL
2. âœ… Run dbt models
3. âœ… Connect to Power BI / Metabase / Grafana
4. ğŸš€ Deploy to cloud (GCP) when ready
