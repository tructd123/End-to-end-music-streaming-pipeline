FROM apache/spark:3.5.0

USER root

# Install Python dependencies
COPY requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir -r /app/requirements.txt

# Copy application code
COPY src/ /app/src/

# Create directories
RUN mkdir -p /app/checkpoints /app/credentials

WORKDIR /app

# Environment variables (override at runtime)
ENV SOURCE_TYPE=pubsub
ENV TRIGGER_INTERVAL="2 minutes"
ENV SPARK_LOG_LEVEL=WARN

# Default command - run GCS streaming job with Pub/Sub
CMD ["spark-submit", \
     "--master", "local[*]", \
     "--packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.17", \
     "--conf", "spark.hadoop.google.cloud.auth.service.account.enable=true", \
     "--conf", "spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem", \
     "/app/src/streaming_to_gcs_pubsub.py"]
